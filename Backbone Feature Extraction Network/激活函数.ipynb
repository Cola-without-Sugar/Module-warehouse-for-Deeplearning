{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678dfbe6",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadf393",
   "metadata": {},
   "source": [
    "这个文档主要用于记录pytorch中激活函数的实现方式\n",
    "\n",
    "### Sigmoid函数\n",
    "'''官方实现方式'''\n",
    "\n",
    "`torch.sigmoid(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数表达式：f(x) = 1/(1+e^-x)\n",
    "# 函数特点：\n",
    "# 优点：1.输出[0,1]之间；2.连续函数，方便求导。\n",
    "# 缺点：1.容易产生梯度消失；2.输出不是以零为中心；3.大量运算时相当耗时（由于是幂函数）。\n",
    " \n",
    "# 函数定义：\n",
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a2530",
   "metadata": {},
   "source": [
    "### tanh函数\n",
    "\n",
    "**官方实现**\n",
    "\n",
    "`torch.tanh(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数表达式：f(x) = (e^x-e^-x)/(e^x+e-x)\n",
    "# 函数特点：\n",
    "# 优点：1.输出[-1,1]之间；2.连续函数，方便求导；3.输出以零为中心。\n",
    "# 缺点：1.容易产生梯度消失； 2.大量数据运算时相当耗时（由于是幂函数）。\n",
    " \n",
    "# 函数定义：\n",
    "def tanh(x):\n",
    "    y = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ea6c3",
   "metadata": {},
   "source": [
    "### ReLU函数\n",
    "**官方实现**\n",
    "\n",
    "`torch.relu(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733ff42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数表达式：f(x) = 1/(1+e^-x)\n",
    "# 函数特点：\n",
    "# 优点：1.输出[0,1]之间；2.连续函数，方便求导。\n",
    "# 缺点：1.容易产生梯度消失；2.输出不是以零为中心；3.大量运算时相当耗时（由于是幂函数）。\n",
    " \n",
    "# 函数定义：\n",
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843836e",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "**官方实现**\n",
    "\n",
    "`torch.nn.LeakyReLU(x,negative_slope,inplace)`\n",
    "\n",
    "* `negative_slope`：控制负激活值的斜率，默认1e-2\n",
    "\n",
    "* `inplace`：是否改变输入数据，如果设置为True，则会直接修改输入数据；如果设置为False，则不对输入数据做修改"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a640336",
   "metadata": {},
   "source": [
    "### ELU\n",
    "**官方实现**\n",
    "\n",
    "`torch.nn.ELU(x,alpha=1.0, inplace=False)`\n",
    "\n",
    "* `alpha`-ELU 公式的α 值。默认值：1.0\n",
    "\n",
    "* `inplace`-可以选择就地执行操作。默认值：False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4328715",
   "metadata": {},
   "source": [
    "### PReLU\n",
    "\n",
    "**官方实现**\n",
    "\n",
    "`torch.nn.PReLU(x,num_parameters=1, init=0.25, device=None, dtype=None)`\n",
    "\n",
    "**参数**\n",
    "\n",
    "* `num_parameters (int)`  需要学习的 a 的数量，尽管作为输入，只有两个值是合法的，1 或者 输入的通道数，默认为 1 \n",
    "* `init (float)`  a的初始值，默认为 0.25。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88cde5",
   "metadata": {},
   "source": [
    "### Softnax\n",
    "对n维输入张量运用Softmax函数，将张量的每个元素缩放到（0,1）区间且和为1。\n",
    "\n",
    "**官方实现**\n",
    "\n",
    "`torch.nn.Softmax(input, dim)`或\n",
    "\n",
    "`torch.nn.functional.softmax(input, dim)`\n",
    "\n",
    "**参数**\n",
    "\n",
    "* `dim` 指明维度，dim=0表示按列计算；dim=1表示按行计算。默认dim的方法已经弃用了，最好声明dim，否则会警告：\n",
    "`UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument`\n",
    "\n",
    "返回结果是一个与输入维度dim相同的张量，每个元素的取值范围在（0,1）区间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9748cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    x = x - np.max(x) # 溢出操作\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd37264",
   "metadata": {},
   "source": [
    "### Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a39e3f",
   "metadata": {},
   "source": [
    "### Hardswish\n",
    "**官方实现**\n",
    "\n",
    "`torch.nn.Hardswish(inplace=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade329b5",
   "metadata": {},
   "source": [
    "### Maxout\n",
    "\n",
    "官方暂未复现方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f5ae1",
   "metadata": {},
   "source": [
    "### Softplus\n",
    "\n",
    "**官方实现**\n",
    "\n",
    "`torch.nn.Softplus(beta=1, threshold=20)`\n",
    "\n",
    "**参数**\n",
    "\n",
    "* `beta` Softplus 配方的β值。默认值：1\n",
    "\n",
    "* `threshold`高于此值的值恢复为线性函数。默认值：20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088deffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
