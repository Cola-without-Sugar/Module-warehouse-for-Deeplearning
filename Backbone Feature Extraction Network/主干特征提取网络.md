# 主干特征提取网路

这一部分主要对主干特征提取网络进行一个大方向的整理，我们要构造一个最基本的卷积神经网络需要对卷积层+激活函数+归一化组成的卷积块有一个了解。（关于分类改进的情况使用不同颜色的 **|** 进行标注，可以对相关改进部分进行索引）

## CNN（Convolutional Neural Networks）

### 卷积层

>主要参考博客[不同卷积讲解](https://blog.csdn.net/xhtchina/article/details/118698170)

> 推荐一篇详细讲解卷积的论文：A guide to convolution arithmetic for deep learning

卷积层设计的主要特点有以下几种：

![卷积层](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303221027665.png)   

**输入数据分割**	 主要包含对输入数据做分割的方式，如滑动窗口输入 <font color='A2CD5A'>**|**</font>

**卷积核设置** 		主要包括对卷积核权重的大小尺寸和通道数的调整 <font color='red'>**|**</font>

**输出数据拼接**	 对输出数据按照一定的规则进行拼接变换。<font color='cornflowerblue'>**|**</font>





#### 原始卷积 (Vanilla Convolution)

普通卷积层是实现卷积神经网络的基础，由卷积层的权重和输入输出组成。卷积运算实际上是对图像进行互相关运算，使用卷积核K对图像逐像素做运算。

使用了爱因斯坦简单式进行实现，同时使用unfold对输入进行快速滑动窗口分割，方便实现卷积。
![卷积层详解](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303220943489.jpg)

#### 跨步卷积 (Stride Convolution) <font color='A2CD5A'>**|**</font>

跨步卷积主要改进的地方为，对滑动窗口的步幅进行调整，从原先的固定为1，转换为移动stride长度。

**跨步卷积去除了部分冗余且缩小了输出的尺寸，一定程度上取到了池化层的作用**

#### 空洞卷积 (Atrous Convolution) <font color='red'>**|**</font> 

> 论文：Multi-Scale Context Aggregation by Dilated Convolutions[1]

空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中相邻两个值内填充dilation rate-1个0。

![空洞卷积](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303221313269.png)

上图为`dilation rate = 2 `的情况。

**空洞卷积的作用**

1. 扩大感受野

> 一般来说，在深度神经网络中增加感受野并且减少计算量的方法是下采样。但是下采样牺牲了空间分辨率和一些输入的信息。
>
> 空洞卷积一方面增大了感受野可以检测分割大目标，另一方面相较于下采样增大了分辨率可以精确定位目标。



1. 捕获多尺度上下文信息

> 当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息



**空洞卷积的缺点**

1. 网格效应

> 仅仅多次叠加 dilation rate 相同的相同尺寸卷积核，会导致kernel 不连续，进而导致不是所有的像素都用来计算了，因此这会损失信息的连续性。
>
> ![网格效应](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303221332650.png)

如图所示，白色网格相当于输入的图片，然后蓝色组成的那个框框就是卷积核，红色的点代表正在处理的像素点，可以看到红色点的旁边那一圈白色的像素点从头到尾都没有被利用到（都被乘0），所以红色那一点卷积后没有考虑到旁边这些白点的信息。



1. 远距离的信息可能不相关

> 如果光采用大的扩张率的卷积可能只对一些大物体分割有效果，那么对小目标来说就不太友好了。因此设计好空洞卷积层的关键在于如何同时处理不同大小物体的关系。

**空洞卷积的改进**

HDC:(混合空洞卷积)

> 1、叠加卷积的 dilation rate 不能有大于1的公约数，防止出现网格效应。比如[2，4，8]不行。
>
> 2、将扩张率设计成锯齿状结构，如[1,2,5,1,2,5]



#### 分组卷积 (Group Convolution) <font color='red'>**|**</font>

> 论文：ImageNet Classification with Deep Convolutional Neural Networks[2]

分组卷积最早是在AlexNet[2]中提及,最早是作为工程实现被提及，下图展示了普通卷积和分组卷积的区别。

![分组卷积合](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303231015225.png)

由图上可以知道，分组卷积的特点是通过分割卷积核组的通道数减少参数的计算量。在分组数为2的情况下由原来的$h\times w\times C\times C'$变为了$h\times w\times C/2\times C'/2\times 2$ 参数变为了一半。

**分组卷积的作用**

1. 减少了参数量并且便于并行运算

2. > 在某些情况下，分组卷积能带来的模型效果确实要优于标准的2D 卷积，是因为组卷积的方式能够增加相邻层filter之间的对角相关性，而且能够减少训练参数，不容易过拟合，这类似于正则的效果。

   <div align=center><img src="https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303231026849.png" alt="分组卷积的作用" style="zoom:200%;" /></div>

> 人们注意到过滤器组似乎始终将卷积层分成两个独立且不同的任务：黑白滤镜和彩色滤镜。
>
> [分组卷积的效果探讨](https://blog.yani.ai/filter-group-tutorial/) 通过查看相邻层卷积组的相关性表明卷积层组的作用是在**在通道维度上使用块对角线结构稀疏性进行学习**



**分组卷积存在的问题**

1. 实验表明分组卷积当分组数 = 2 时精度和参数量都有所提高。但是仍然存在一些问题尚未解决：

> 如何决定使用的卷积组的数量？卷积组之间可以重叠吗？是否所有组都必须具有相同的大小，异构卷积组又如何呢？

2. 数据信息只在本组里面存在，通道之间的信息没有流通，存在信息的屏蔽与阻塞，从而会丢失全局通道的信息。

**分组卷积的改进**

背景：原始的组卷积实现中，不同通道的特征会被划分到不同的组里面，直到网络的末端才将其融合起来，中间过程显然缺乏信息的交互（考虑到不同滤波器可提取到不同的特征）。

为了解决此问题。ShuffleNet结合逐点组卷积(Pointwise Convolution,PGC) 和 通道混洗 (channel shuffle)，来实现一个高效经量化的移动网络设计。提出了Shuffle单元。



#### 深度(可分离)卷积 (Depthwise Separable Convolution) <font color='red'>**|**</font> 

> 论文：《Xception: Deep Learning with Depthwise Separable Convolutions》[3] (Accepted by CVPR 2017)

深度可分离卷积，由深度卷积(Depthwise Convolution)和逐点卷积(Pointwise Convolution)两部分组成，后也被MobileNet等著名网络大规模应用。标准的卷积过程中对应图像区域中的所有通道均被同时考虑，而深度可分离卷积打破了这层瓶颈，将通道和空间区域分开考虑，对不同的输入通道采取不同的卷积核进行卷积，它将普通的卷积操作分解为两个过程，目的是希望能用较少的参数学习更丰富的特征表示。

![深度可分离卷积](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303231243199.png)

深度可分离卷积的实现如下图所示，首先构建深度卷积 (Depthwise conv) 对输入的每个通道有一个负责的卷积核，因此，深度卷积的输出特征图数量等于输入特征图数量，无法进行有效的维度扩展。

由于一个特征图仅被一个滤波器卷积，无法有效的利用不同通道在相同空间位置上的特征信息，由此加入了逐点卷积 (Pointwise conv)。点卷积主要是要1×1卷积构成，负责将深度卷积的输出按通道投影到一个新的特征图上。

![深度可分离卷积1](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303231244515.png)

**深度可分离卷积的优点：**

1. **降低参数量和计算量**  深度可分离卷积将原始的卷积运算分为两层，一层用于滤波（深度卷积），一层用于组合（逐点卷积）。这种分解过程能极大减少模型的参数量和计算量。

**深度可分离卷积的缺点：**

1. **降低模型容量** 深度可分离卷积在应用时并没有使用激活函数。此外，虽然深度可分离卷积可以显著的降低模型的计算量，但同时也会导致模型的容量显著降低，从而导致模型精度的下降。



#### 转置卷积

> 论文：[Fully Convolutional Networks for Semantic Segmentation](https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)[4]

转置卷积（transposed Convolutions）又名反卷积（deconvolution）或是分数步长卷积（fractially straced convolutions）。反卷积的概念第一次出现是Zeiler在2010年发表的论文Deconvolutional networks中。一般应用在编解码结构中的解码器部分或者DCGAN中的生成器中等。因为数字信号中有反卷积的概念而转置卷积通常只能恢复卷积的形状 (shape) 而不能恢复 值 (value) 。所以不能真正意义上称为反卷积。这一概念是在图像分割领域的里程碑之作 FCN 中得到关注。

![反卷积计算方式](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202303231349155.png)

转置卷积即是对卷积的运算过程进行逆运算，用于输出1对多的输出特征图。具体的公式的介绍可以看这个博文[转置卷积的推导](https://blog.csdn.net/qq_39478403/article/details/121181904)

**转置卷积的优点：**

1. **特征上采样** 利用转置卷积，可以引入参数让网络自动学习卷积核的权重以更好地恢复空间分辨率。一般来说，利用转置卷积来替代常规的上采样操作（最近邻插值、双线性插值即双立方插值）会取得更好的效果（在没有过拟合的情况下）
2. **特征可视化** 利用转置卷积还可以对特征图进行可视化。有时间的强烈推荐大家去阅读原论文《Visualizing and Understanding Convolutional Networks》，有助于帮助大家理解不同深度的各个特征图究竟学到了什么特征。比如，增加网络的深度有利于提取更加抽象的高级语义特征，而增加网络的宽度有利于增强特征多样性的表达。或者是小的卷积核有利于特征的学习，而小的步长则有利于保留更多的空间细节信息。



**转置卷积的缺点：**

1. **增加了参数量** 利用转置卷积进行学习上采样，增加了整个网络的学习权重的数量。
2.  **棋盘效应** 在进行上采样时，如果步长和卷积核尺寸选择不当，极易出现网格效应。具体可与参考[棋盘效应产生的原因及解决方案](https://zhuanlan.zhihu.com/p/548904297)



#### 1×1 卷积 

#### 可变形卷积

#### 空间可分离卷积

#### 图卷积

#### Inception 模块

#### 非对称卷积

#### Octave卷积

#### Het卷积

#### Cond卷积

#### 动态卷积

#### Ghost模块

#### 自校正卷积

#### Do-卷积

#### ResNeSt Block

#### 内卷(Involution)



### 激活函数

**激活函数（Activation Function）**是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。类似于人类大脑中基于神经元的模型，激活函数最终决定了要发射给下一个神经元的内容。

![激活函数](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304031033849.png)

假设激活函数$$h(x)$$是一个激活函数。当我们的n趋近于正无穷，激活函数的导数趋近于零，我们称之为右饱和，反之称之为左饱和。当激活函数满足这两个条件时，我们称之为饱和激活函数，反之称之为非饱和激活函数。

**饱和激活函数** <font color='red'>**-|**</font>

**非饱和激活函数** <font color='cornflowerblue'>**-|**</font>



#### Sigmoid函数 <font color='red'>**-|**</font>

![sigmoid函数](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304031045899.png)

**特点：**它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.

**优点：**

* 非常适合用于将预测概率作为输出的模型
* 梯度平滑，避免出现跳跃的输出值
* 将输出转移到[0,1]的区间内，一定程度上起到了归一化的作用

**缺点：**

* 容易出现梯度消失问题(gradient vanishing)，导致部分参数无法更新
* 函数的输出并不是zero-centered，会导致模型的收敛速度变慢，模型的更新方向单一

![更新方向](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304031829329.png)

* 幂运算相对来讲比较耗时



#### tanh函数 <font color='red'>**-|**</font>

![tanh函数](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304031831442.png)

**特点**

- 首先，当输入较大或较小时，输出几乎是**平滑的**并且**梯度较小**，这**不利于权重更新**。二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。



#### ReLU <font color='cornflowerblue'>**-|**</font>

![image-20230403184436030](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304031844389.png)

近年来，ReLU函数变得越来越受欢迎。全称是Rectified Linear Unit，中文名字：修正线性单元。ReLU是Krizhevsky、Hinton等人在2012年《ImageNet Classification with Deep Convolutional Neural Networks》论文中提出的一种线性且不饱和的激活函数。

ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：

* 解决了gradient vanishing问题 (在正区间)

* Sigmoid和Tanh激活函数均需要计算指数， 复杂度高， 而ReLU只需要一个阈值即可得到激活值。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。计算速度非常快，只需要判断输入是否大于0。
  收敛速度远快于sigmoid和tanh

* ReLU的非饱和性可以有效地解决梯度消失的问题， 提供相对宽的激活
  边界。

* ReLU的单侧抑制提供了网络的稀疏表达能力。

ReLU也有几个需要特别注意的问题：

* ReLU 函数的输出为 0 或正数，不是zero-centered
* **Dead ReLU Problem**，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。这是由于函数导致负梯度在经过该ReLU单元时被置为0， 且在之后也不被任何数据激活， 即流经该神经元的梯度永远为0， 不对任何数据产生响应。 当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见； (2) learning rate太高导致在训练过程中参数更新太大，会导致超过一定比例的神经元不可逆死亡， 进而参数梯度无法更新， 整个训练过程失败。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。
          尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！



#### Leaky ReLU <font color='cornflowerblue'>**-|**</font>

人们为了解决**Dead ReLU Problem**，提出了将ReLU的**前半段**设为**ax**而非0。而a通常取0.01

![leaky ReLU](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304041321040.png)

为什么 Leaky ReLU 比 ReLU 更好？

* Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题；

* leak 有助于扩大 ReLU 函数的范围，通常 a 的值为 0.01 左右；

* Leaky ReLU 的函数范围是（负无穷到正无穷）。

但另一方面， a值的选择增加了问题难度， 需要较强的人工先验或多次重复训练以确定合适的参数值。

> **注意：**从理论上讲，Leaky ReLU 具有 ReLU 的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明 Leaky ReLU 总是比 ReLU 更好。



#### ELU (Exponential Linear Units) <font color='cornflowerblue'>**-|**</font>

ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。

![ELU](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304041331583.png)

显然，ELU 具有 ReLU 的所有优点，并且：

* 没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；

* ELU 通过减少偏置偏移的影响，使**正常梯度**更接近于**单位自然梯度**，从而使**均值向零加速学习**；

* ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。

一个小问题是它的**计算强度更高**。与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。



#### PReLU（Parametric ReLU）<font color='cornflowerblue'>**-|**</font>

![PRELU](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202304061039412.png)

PReLU 也是 ReLU 的改进版本,想法是基于参数的方法，即Parametric ReLU:，其中 $$\alpha $$ 可由back propagation学出来。

> Kaiming He的论文《Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification》指出，不仅可以训练，而且效果更好。

PReLU 的优点如下：

- 在负值域，PReLU 的斜率较小，这也可以避免 Dead ReLU 问题。
- 与 ELU 相比，PReLU 在负值域是线性运算。尽管斜率很小，但不会趋于 0。



#### Softmax <font color='cornflowerblue'>**-|**</font>

函数表达式：
$$
{{{e^{{z_i}}}} \over {\sum\nolimits_{j = 1}^K {{e^{{z_j}}}} }}
$$
 **Softmax 是用于多类分类问题的激活函数**，在多类分类问题中，超过两个类标签则需要类成员关系。对于**长度为 K** 的任意实向量，Softmax 可以将其压缩为**长度为 K，值在（0，1）范 围内，并且向量中元素的总和为 1** 的实向量。

  Softmax 与正常的 max 函数不同：max 函数仅输出最大值，但 Softmax 确保较小的值具有较小的概率，并且不会直接丢弃。我们可以认为它是 argmax 函数的概率版本或「soft」版本。

Softmax 函数的分母结合了原始输出值的所有因子，这意味着 Softmax 函数获得的各种概率彼此相关。

Softmax 激活函数的主要缺点是：

* 在零点不可微；
* 负输入的梯度为零，这意味着对于该区域的激活，权重不会在反向传播期间更新，因此会产生永不激活的死亡神经元。
  



#### Swish <font color='cornflowerblue'>**-|**</font>

函数表达式：y = x * sigmoid (x)

Swish 的设计受到了 LSTM 和高速网络中 gating 的 sigmoid 函数使用的启发。我们使用相同的 gating 值来简化 gating 机制，这称为 self-gating。

self-gating 的优点在于它只需要简单的标量输入，而普通的 gating 则需要多个标量输入。这使得诸如 Swish 之类的 self-gated 激活函数能够轻松替换以单个标量为输入的激活函数（例如 ReLU），而无需更改隐藏容量或参数数量。
    
Swish 激活函数的主要优点如下：

* 「无界性」有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和；（同时，有界性也是有优势的，因为有界激活函数可以具有很强的正则化，并且较大的负输入问题也能解决）；
  导数恒 > 0；

* 平滑度在优化和泛化中起了重要作用。



#### Maxout <font color='cornflowerblue'>**-|**</font>

这个函数可以参考论文《maxout networks》，Maxout是深度学习网络中的一层网络，就像池化层、卷积层一样等，我们可以把maxout 看成是网络的激活函数层，我们假设网络某一层的输入特征向量为：X=（x1,x2,……xd），也就是我们输入是d个神经元。Maxout隐藏层每个神经元的计算公式如下：

> https://blog.csdn.net/weixin_39910711/article/details/114849349



#### Softplus <font color='cornflowerblue'>**-|**</font>

softplus的函数为：
$$
f(x) = \ln (1 + {e^x})
$$
Softplus 函数类似于 ReLU 函数，但是相对较平滑，像 ReLU 一样是单侧抑制。它的接受范围很广：(0, + inf)



### 归一化



#### BN



#### CN



#### GN



### 池化层



#### 全局平均池化



#### 最大池化







## Transformer





## 附录

### 1、参考文献

[1]Yu F , Koltun V . Multi-Scale Context Aggregation by Dilated Convolutions[C]// ICLR. 2016.

[2] Krizhevsky A ,  Sutskever I ,  Hinton G . ImageNet Classification with Deep Convolutional Neural Networks[J]. Advances in neural information processing systems, 2012, 25(2).

[3]Chollet F . Xception: Deep Learning with Depthwise Separable Convolutions[J]. IEEE, 2017.

[4] Long J , Shelhamer E , Darrell T . Fully Convolutional Networks for Semantic Segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 39(4):640-651.
