# 深度学习中的模块信息

**说明：**此文档针对深度学习中的一些论文阅读得到的一些有趣的思路与模块想法总结而成，旨在能够对模型组成部分有一些了解，并分析各个模块之间对于性能之间提升的原因和思路的探索，文章来源会标注在附录部分，主要的模块思想会整理成文档的形式。

根据一些论文的思想判断，深度学习的基础框架如图所示：

![深度学习中的模块化改进](https://cdn.jsdelivr.net/gh/Cola-without-Sugar/markdown_img/202302221729277.png)

如图片中所述，图像的模块化处理主要分为对图像的预处理部分、数据集处理部分、主干特征提取网络、权重共享与更新模块、特征融合模块与输出编码分类器模块。

* **图像的预处理**  包括图像增强，图像降噪去噪，图像的去雾去雨算法等对图像做一些基础的分析部分。
* **数据集的划分**  数据集的划分主要涉及，数据集的划分方式，标注方式与精度等方式。
* **主干特征提取网络** 一个基本深度训练网络的架构组成。方向主要有：全局信息与局部信息的感知、针对特定领域的特定模块
* **权重共享与更新模块** 主要涉及对提取信息的共享能力，损失函数决定了网络权重的优化方向。优化器则是寻找模型最优点的能力。
* **特征融合模块** 主要涉及到图像对于低层语义的感知能力
* **输出编码分类器** 主要涉及到对于具体分类任务的优化





## 图像的预处理

图像的预处理模块是指对取得的原始图像进行初步的增强，保证网络学习到特征的质量。有各种相关的方向研究。



### 图像滤波

图像的滤波是指去除掉图像上的噪声信息。使得图像更加平滑，去除掉由设备等原因造成的噪声异常点后，能够增强像素之间的连续性。

常用的去除噪声的方法主要有高斯滤波。





























### 数据增强

数据增强的目的实际上是为了能够检验模型的鲁棒性，数据增强可以有效的提升模型的检测能力。[<font color='cornflowerblue'>**补充一个能够说明数据增强对于神经网络影响的文章**</font>] 

> 神经网络对于图像特征学习的理解很有可能是懒惰的，它往往只学习那些有助于分类的特征，从而对一些更深层次的特征忽略。比如如果一个青色的苹果与黄色的橙子，神经网络往往学习到颜色是区分它们的主要特征，继而实现高精度的分类，但实际上形状也是重要依据，当给出青色的橙子时，网络就会犯错。

所以数据增强本质上就是给神经网络增加困难，使得网络能够关注分类的本质特征。

> https://zhuanlan.zhihu.com/p/41679153 数据增强的方法

一个标准是我们要尽可能的选取在自然界中更加常见的数据增强方式。减少不合理性。



















### 特征增强

图像的去雾去雨算法是指对图像本身不明显的特征进行增强，去除环境对图像的影响。利用图像中隐藏的信息，比如光照，饱和度，亮度等对图像中不明显的特征进行增强。































## 数据集的划分

数据集的划分方法和标注方式。根据数据集的来源与标签数量的不同，我们可以分为监督学习，半监督学习，无监督学习，弱监督学习与自监督学习。极端一点的是样本量与数据量同样很少的少样本学习(Few-shot learning)

|                  | 数据量少          | 数据量多   |
| ---------------- | ----------------- | ---------- |
| **无标签**       |                   | 无监督学习 |
| **有任务**       | 元学习            | 自监督学习 |
| **标签量少**     | 少样本学习        | 半监督学习 |
| **标签量多**     |                   | 监督学习   |
| **数据传入方式** | 在线学习/增量学习 | 离线学习   |



> [自监督与无监督的区别](https://www.likecs.com/show-204753841.html#sc=900)

### 数据集处理

































### 标签处理

































## 主干特征提取网络

主干特征提取网络是模型的核心部分，影响模型对于数据特征的识别与利用的能力。目前视觉部分主要的主干特征提取网络主要有CNN与Transformer。



### CNN（Convolutional Neural Networks）

> [卷积神经网络的发展](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247560258&idx=1&sn=11ae62fdf4c06bd2901937aac157bc0e&chksm=e9e0e5c9de976cdf054c7554cf42c5f314ed9dd10b5f357584c3fd037818df8427236de6610f&scene=27 )

卷积神经网络的核心是卷积模块，由卷积层 + 归一化模块 + 激活函数 + 池化层构成。所以主要的改进方向是从这几个方面深入。

#### 卷积算子

##### 解耦算子

**背景：**CNN学习的特征是自然解耦的启发，从解耦的角度来看，原始的cnn做了一个强有力的假设，即类内变异可以通过规范的乘法线性建模，语义差异用角度的余弦来描述。然而，这种建模方法并不一定是适合所有任务的最佳方法。使用解耦学习框架，我们可以根据任务本身设计解耦操作符，也可以直接从数据中学习它们。

**改进：**提出了两种不同的解耦卷积算子：有界算子和无界算子，有界算子可能在对抗性攻击时产生更快的收敛性和更好的鲁棒性，而无界算子可能具有更好的表示能力。

**介绍：**























### Transformer

Transformer是由NLP领域的注意力机制迁移过来的网络模型。早期的Transformer是跟神经网络结构一同使用的，后来随着注意力模块的发展，逐渐摒弃了传统的卷积模块，变成纯注意力机制的模块。相比于传统的卷积神经网络，Transformer的参数量更多且更精确，同时计算量也随着增加，模型也更加“笨重“。





## 权重共享与更新模块

权重共享与更新模块主要包括。初始化权重的选择、权重的更新方法、损失函数的构建、权重共享的机制。



### 初始化权重的选择

初始化权重决定了模型训练的初始状态，随机初始状态的权重对于模型来说很难进一步优化。而一个合理的权重分布可以帮助模型更快的到达最优点。



### 权重的更新方法

权重的更新方法，主要是指在网络优化过程中一些超参数的计算，这些优化器往往决定了最终模型的表现质量。



### 损失函数的构建

损失函数指定了网络优化的目标，最小化损失函数是模型在进行参数更新时的方向。在此上有很多方向的改进，如增加优化器全局表示的能力等。



### 权重共享机制

权重共享机制是指神经网络在训练的时候多个神经网络共享神经网络权重的机制。一种模型间的权重共享机制提出是因为将模型迁移到其他数据不同上的数据时，希望保持网络原先的性能加快训练。另一种则是希望能够将数据统一进行权重调整（类似于各种归一化的手段）





## 特征融合模块

特征融合模块是指在神经网络上对不同层次的输出特征图进行拼接，希望能够获得更好的检测结果。最开始是由目标检测网络提出的。

* 基于熵-峰度的高特征值(EKbHFV) 和基于元启发式的修正遗传算法（MGA）特征选择技术[*]([A decision support system for multimodal brain tumor classification using deep learning | SpringerLink](https://link.springer.com/article/10.1007/s40747-021-00321-0))

  > 从模型的平均池化层中提取特征但是并不适用于分类，所以使用上述的技术进行特征选择后，通过基于非冗余的串行的方法融合特征最后通过多类SVM三次分类器进行分类。

* 一个基于注意力的编码-解码器网络，它将视觉、语言和位置特征的多模态信息结合在一起。通过使用注意机制来关注问题的关键特征[**](https://doi.org/10.1016/j.patcog.2021.108214)













## 输出编码分类器

输出编码分类器模块，是针对已经在神经网络上提取好高维度的特征编码信息后，将编码信息执行多种下游任务的处理模块。常用的分类器有softmax方法及其他改进的方式。



## 下游任务模块

### 图像分类

### 语义分割

### 目标检测

### 实例分割



## 附录

### 1 其他领域有趣的模块方法

### 2 想法

* 权重的更新影响了模型到达最优点的能力，较小的学习率很容易使模型掉入局部最优点，那么能否在模型更新的过程中加入遗传算法，使得模型获得一定程度的变异，使其跳出局部最优路径，寻找全局最优路径。

### 3 参考文献



